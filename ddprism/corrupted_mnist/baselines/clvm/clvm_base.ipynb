{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afbd1db-a00f-439e-af61-388eb31e0432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport config_base_grass\\nimport config_base_mnist\\nimagenet_path = '/mnt/home/aakhmetzhanova/ceph/galaxy-diffusion/corrupted-mnist/dataset/grass_jpeg/'\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ml_collections import config_flags\n",
    "import numpy as np\n",
    "from absl import app, flags\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import math, os\n",
    "from typing import Any, Callable, Mapping, Sequence, Tuple, Optional\n",
    "from jax import Array\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, orbax_utils\n",
    "import optax\n",
    "import wandb\n",
    "from orbax.checkpoint import CheckpointManager, PyTreeCheckpointer\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "from ddprism import training_utils\n",
    "from ddprism.corrupted_mnist import datasets\n",
    "from ddprism.metrics import metrics, image_metrics\n",
    "\n",
    "'''\n",
    "import config_base_grass\n",
    "import config_base_mnist\n",
    "imagenet_path = '/mnt/home/aakhmetzhanova/ceph/galaxy-diffusion/corrupted-mnist/dataset/grass_jpeg/'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7998868-7aa0-4b63-aa4e-1f560fefdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cLVM(nn.Module):\n",
    "    r\"\"\"Creates an instance of cLVM model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: Model used for cLVM method. \n",
    "    \"\"\"\n",
    "    model: nn.Module\n",
    "\n",
    "    def sample_latents(\n",
    "        self, rng: Array, model: nn.Module, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        # Sample latent variables for the target and background datasets.\n",
    "        rng_zx, rng_zy, rng_tx = jax.random.split(rng, 3)\n",
    "        \n",
    "        # Compute mean and std for the target and background datasets.\n",
    "        mu_zx, log_sigma_zx, mu_tx, log_sigma_tx = model.encode_target(x, a_mat)\n",
    "        mu_zy, log_sigma_zy = model.encode_bkg(x, a_mat)\n",
    "        \n",
    "        # Sample latent variables corresponding to the enriched signal in the target dataset.\n",
    "        eps_tx = jax.random.normal(rng_tx, shape=x.shape[0] + (model.target_latent_dim,))\n",
    "        tx = mu_tx + jax.exp(log_sigma_tx) * eps_tx\n",
    "\n",
    "        # Sample latent variables corresponding to the background in the target dataset.\n",
    "        eps_zx = jax.random.normal(rng_zx, shape=x.shape[0] + (model.bkg_latent_dim,))\n",
    "        zx = mu_zx + jax.exp(log_sigma_zx) * eps_zx\n",
    "\n",
    "        # Sample latent variables corresponding to the background in the background dataset.\n",
    "        eps_zy = jax.random.normal(rng_zy, shape=y.shape[0] + (model.bkg_latent_dim,))\n",
    "        zy = mu_zy + jax.exp(log_sigma_zy) * eps_zy\n",
    "        \n",
    "        return tx, zx, zy\n",
    "\n",
    "    def expect_data(\n",
    "        self, model: nn.Module, tx: Array, zx: Array, zy: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        # Compute expected values of the target and background datasets.\n",
    "        x = model.decode_bkg(zx, a_mat) + model.decode_target(tx, a_mat)\n",
    "        y = model.decode_bkg(zy, a_mat)\n",
    "        return x, y\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, rng: Array, model: nn.Module, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        # Generate new samples of the data.\n",
    "        # Sample latent variables.\n",
    "        tx, zx, zy = self.sample_latents(rng, model, x, y, a_mat)\n",
    "        # Compute expectation for the data, given the latents.\n",
    "        x, y = self.expect_data(model, tx, zx, zy, a_mat)\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2859d7f8-a722-4a76-99cb-d350d5b1796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clvmLinear(nn.Module):\n",
    "    r\"\"\"Creates an instance of cLVM model that linearly maps latent variables to the observed space.\n",
    "    \"\"\"\n",
    "\n",
    "    def sample_latents(\n",
    "        self, rng: Array, model_params: Tuple, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        pass\n",
    "\n",
    "    def expect_data(\n",
    "        self, model_params: Tuple, tx: Array, zx: Array, zy: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        \n",
    "        w_mat, s_mat, mu_x, mu_y = model_params\n",
    "        # Compute expected values of the target and background datasets.\n",
    "        x = s_mat @ zx + w_mat @ tx + mu_x\n",
    "        y = s_mat @ zy + mu_y\n",
    "\n",
    "        if a_mat is not None:\n",
    "            x = a_mat @ x\n",
    "            y = a_mat @ y\n",
    "        return x, y\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, rng: Array, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple: \n",
    "        # Following GaussianDenoiserDPLR model?\n",
    "        # Initialize model parameters.\n",
    "        mu_x = self.param(\n",
    "            \"mu_x\", lambda rng, shape: jnp.ones(shape), x.shape[-1:]\n",
    "        )\n",
    "\n",
    "        mu_y = self.param(\n",
    "            \"mu_x\", lambda rng, shape: jnp.ones(shape), y.shape[-1:]\n",
    "        )\n",
    "        \n",
    "        s_mat = self.param(\n",
    "            \"s_mat\",\n",
    "            lambda rng, shape: ...,\n",
    "            (..., )\n",
    "        )\n",
    "\n",
    "        w_mat = self.param(\n",
    "            \"w_mat\",\n",
    "            lambda rng, shape: ...,\n",
    "            (..., )\n",
    "        )\n",
    "\n",
    "        model_params = (w_mat, s_mat, mu_x, mu_y)\n",
    "        \n",
    "        # Generate new samples of the data.\n",
    "        # Sample latent variables.\n",
    "        tx, zx, zy = self.sample_latents(rng, model, x, y, a_mat)\n",
    "        \n",
    "        # Compute expectation for the data, given the latents.\n",
    "        x, y = self.expect_data(model_params, tx, zx, zy, a_mat)\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bfdb574-aa31-4ecf-a181-3ecc55096357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clvmVAE(clvmLinear):\n",
    "    r\"\"\"Creates an instance of cLVM model that non-linearly maps latent variables to the observed space.\n",
    "    \"\"\"\n",
    "    vae_z: nn.Module\n",
    "    vae_t: nn.Module\n",
    "\n",
    "    def sample_latents(\n",
    "        self, rng: Array, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        # Sample latent variables for the target and background datasets.\n",
    "        rng_zx, rng_zy, rng_tx = jax.random.split(rng, 3)\n",
    "        \n",
    "        # Compute mean and std for the target and background datasets.\n",
    "        mu_tx, log_sigma_tx = self.vae_t.encoder(x, a_mat)\n",
    "\n",
    "        mu_zx, log_sigma_zx = self.vae_z.encoder(x, a_mat)\n",
    "        mu_zy, log_sigma_zy = self.vae_z.encoder(y, a_mat)\n",
    "        \n",
    "        # Sample latent variables corresponding to the enriched signal in the target dataset.\n",
    "        eps_tx = jax.random.normal(rng_tx, shape=x.shape[0] + (model.target_latent_dim,))\n",
    "        tx = mu_tx + jax.exp(log_sigma_tx) * eps_tx\n",
    "\n",
    "        # Sample latent variables corresponding to the background in the target dataset.\n",
    "        eps_zx = jax.random.normal(rng_zx, shape=x.shape[0] + (model.bkg_latent_dim,))\n",
    "        zx = mu_zx + jax.exp(log_sigma_zx) * eps_zx\n",
    "\n",
    "        # Sample latent variables corresponding to the background in the background dataset.\n",
    "        eps_zy = jax.random.normal(rng_zy, shape=y.shape[0] + (model.bkg_latent_dim,))\n",
    "        zy = mu_zy + jax.exp(log_sigma_zy) * eps_zy\n",
    "        \n",
    "        return tx, zx, zy\n",
    "\n",
    "    def expect_data(\n",
    "        self, tx: Array, zx: Array, zy: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple:\n",
    "        \n",
    "        w_mat, s_mat, mu_x, mu_y = model_params\n",
    "        \n",
    "        # Compute expected values of the target and background datasets.\n",
    "        x = self.vae_z.decoder(zx, a_mat) + self.vae_t.decoder(tx, a_mat)\n",
    "        y = self.vae_z.decoder(zy, a_mat)\n",
    "\n",
    "        if a_mat is not None:\n",
    "            x = a_mat @ x\n",
    "            y = a_mat @ y\n",
    "        return x, y\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, rng: Array, x: Array, y: Array, a_mat: Optional[Array] = None\n",
    "    ) -> Tuple: \n",
    "        \n",
    "        # Generate new samples of the data.\n",
    "        # Sample latent variables.\n",
    "        tx, zx, zy = self.sample_latents(rng, x, y, a_mat)\n",
    "        \n",
    "        # Compute expectation for the data, given the latents.\n",
    "        x, y = self.expect_data(tx, zx, zy, a_mat)\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f656bfaa-b551-47f4-a44b-8a05201d332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    \"\"\"Update model with gradients.\"\"\"\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb32cd86-f67d-4ca0-b17a-c9de10af7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(rng, state, x, y, sigma_noise):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "\n",
    "    # loss\n",
    "    def loss_fn(params):\n",
    "        # Draw samples in data space.\n",
    "        x_draws, y_draws = state.apply_fn(\n",
    "            {'params': params}, rng, x, y, \n",
    "            rngs={'dropout': rng_drop} # Not sure if necessary?\n",
    "        )\n",
    "\n",
    "        # Compute loss function\n",
    "        loss = - (optax.losses.squared_error(x, x_draws) / 2 / sigma_noise**2).mean(axis=-1)\n",
    "        loss -= (optax.losses.squared_error(y, y_draws) / 2 / sigma_noise**2).mean(axis=-1)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn,)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "\n",
    "    return grads, loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17672e5d-bba0-4ef9-8064-f40b83d4ddf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-env",
   "language": "python",
   "name": "jax-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
