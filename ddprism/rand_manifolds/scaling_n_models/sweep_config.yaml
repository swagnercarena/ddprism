program: train.py
method: bayes
metric:
  goal: minimize
  name: final_divergence_x_2
parameters:
  lr_init_val:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.01
  epochs:
    values: [8192, 16384, 32768, 65536]
  gaussian_em_laps:
    values: [8, 16, 32]
  diffusion_em_laps:
    values:
      - [16, 32, 32]
      - [16, 32, 64]
      - [16, 64, 128]
      - [16, 96, 192]
  sde.a:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.01
  sde.b:
    distribution: log_uniform_values
    min: 1.0
    max: 100.0
  hidden_features:
    values:
      - [128, 128, 128]
      - [256, 256, 256]
  emb_features:
    values: [32, 64, 128, 256]
  sampling_kwargs.steps:
    values: [256, 1024, 4096, 16384]
  sampling_kwargs.tau:
    distribution: log_uniform_values
    min: 0.001
    max: 0.1
  post_error_threshold:
    values: [1e-1, 1e0, 1e1]
  time_conditioning:
    values: ['concat', 'film']
  dropout_rate:
    values: [0.0, 0.01, 0.1]
  time_mlp_normalize:
    values: [True, False]